{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n",
      "1.14.0\n",
      "Found 3129 images belonging to 10 classes.\n",
      "(16, 768)\n",
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "print(cv2.__version__)\n",
    "print(tf.__version__)\n",
    "assert tf.executing_eagerly() == True\n",
    "\n",
    "class_names = [\"black\", \"blue\", \"brown\", \"gold\", \"green\", \"orange\", \"purple\", \"red\", \"silver\", \"white\"]\n",
    "IMG_HEIGHT = 256\n",
    "IMG_WIDTH = 256\n",
    "IMG_CHN = 3\n",
    "\n",
    "\n",
    "def generate_generator(generator, path, batch_size = 16, img_height = IMG_HEIGHT, img_width = IMG_WIDTH):\n",
    "\n",
    "        gen = generator.flow_from_directory(path,\n",
    "                                            classes = class_names,\n",
    "                                            target_size = (img_height,img_width),\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle=True, \n",
    "                                            seed=7)\n",
    "        while True:\n",
    "            X,y = gen.next()  \n",
    "            data = []\n",
    "            for img in X:\n",
    "                chans = cv2.split(img)\n",
    "                colors = (\"r\", \"b\", \"g\")\n",
    "                features = []\n",
    "                # loop over the image channels\n",
    "                for (chan, color) in zip(chans, colors):\n",
    "                    hist = cv2.calcHist([chan], [0], None, [256], [1, 256])\n",
    "                    features.extend(hist)\n",
    "                features = np.squeeze(np.array(features))\n",
    "                data.append(features)\n",
    "            data = np.array(data)/100.0\n",
    "            print(data.shape)\n",
    "            yield data, y #Yield both images and their mutual label\n",
    "                \n",
    "class Dataloader:\n",
    "    def __init__(self, data_path,  batch_size = 16):\n",
    "        \n",
    "       \ttrain_imgen = keras.preprocessing.image.ImageDataGenerator(rotation_range = 10,\\\n",
    "\t\t\t\t\t\t\t\t\t\twidth_shift_range = 0.35, height_shift_range = 0.35,\\\n",
    "\t\t\t\t\t\t\t\t\t\thorizontal_flip = True)\n",
    "\n",
    "        test_imgen = keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "        self.train_generator = generate_generator(train_imgen,\n",
    "                                               path = str(data_path) + \"train/\",\n",
    "                                               batch_size=batch_size)       \n",
    "\n",
    "        self.val_generator = generate_generator(test_imgen,\n",
    "                                              path = str(data_path)+ \"val/\",\n",
    "                                              batch_size=batch_size)              \n",
    "\n",
    "        \n",
    "    def load_image(self, val = False):\n",
    "        if val:\n",
    "            return next(self.val_generator)\n",
    "        else:\n",
    "            return next(self.train_generator)\n",
    "    \n",
    "    def load_dl(self):\n",
    "        return [self.train_generator, self.val_generator]\n",
    "    \n",
    "    \n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "\n",
    "    #inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    #inp = std * inp + mean\n",
    "    #inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "dl = Dataloader(\"./mixed_vehicle/\")\n",
    "inputs, labels = dl.load_image()\n",
    "classes = [np.where(r==1)[0][0] for r in labels]\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "def CarColorClassificationNet(): \n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Input(shape = (256*3), dtype = 'float32', name = \"input\" ))\n",
    "    model.add(layers.Dense(32, activation= 'relu', name = \"fc1\", kernel_initializer = 'glorot_uniform'))\n",
    "    model.add(layers.Dense(10, activation= 'softmax', name = \"output\", kernel_initializer = 'glorot_uniform'))\n",
    "    \n",
    "    model.compile(optimizer = keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "#create DNN model\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "trainset_size = 3087\n",
    "valset_size = 575\n",
    "epochs = 100\n",
    "dl = Dataloader(\"./mixed_vehicle/\")\n",
    "train_generator, val_generator = dl.load_dl()\n",
    "checkpoint_path = \"training_hist_new/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "\n",
    "model = CarColorClassificationNet()\n",
    "keras.utils.plot_model(model, 'vehicle_color_classification.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3087 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.7505 - categorical_accuracy: 0.4535Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00001: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 301ms/step - loss: 1.7476 - categorical_accuracy: 0.4538 - val_loss: 1.5613 - val_categorical_accuracy: 0.4974\n",
      "Epoch 2/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.2740 - categorical_accuracy: 0.5479Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00002: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 55s 287ms/step - loss: 1.2747 - categorical_accuracy: 0.5478 - val_loss: 1.4584 - val_categorical_accuracy: 0.5287\n",
      "Epoch 3/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.1827 - categorical_accuracy: 0.5934Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00003: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 288ms/step - loss: 1.1831 - categorical_accuracy: 0.5938 - val_loss: 1.2707 - val_categorical_accuracy: 0.5843\n",
      "Epoch 4/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.1269 - categorical_accuracy: 0.6159Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00004: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 289ms/step - loss: 1.1257 - categorical_accuracy: 0.6158 - val_loss: 1.4331 - val_categorical_accuracy: 0.5826\n",
      "Epoch 5/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.1325 - categorical_accuracy: 0.6058Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00005: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 289ms/step - loss: 1.1330 - categorical_accuracy: 0.6058 - val_loss: 1.5037 - val_categorical_accuracy: 0.5774\n",
      "Epoch 6/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.1115 - categorical_accuracy: 0.6090Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00006: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 288ms/step - loss: 1.1119 - categorical_accuracy: 0.6090 - val_loss: 1.3081 - val_categorical_accuracy: 0.5704\n",
      "Epoch 7/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.1167 - categorical_accuracy: 0.6182Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00007: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 1.1138 - categorical_accuracy: 0.6190 - val_loss: 1.4994 - val_categorical_accuracy: 0.5791\n",
      "Epoch 8/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0734 - categorical_accuracy: 0.6240Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00008: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 289ms/step - loss: 1.0699 - categorical_accuracy: 0.6252 - val_loss: 1.1786 - val_categorical_accuracy: 0.6278\n",
      "Epoch 9/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0999 - categorical_accuracy: 0.6335Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00009: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 289ms/step - loss: 1.0995 - categorical_accuracy: 0.6343 - val_loss: 1.1858 - val_categorical_accuracy: 0.6400\n",
      "Epoch 10/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0427 - categorical_accuracy: 0.6432Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00010: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 291ms/step - loss: 1.0448 - categorical_accuracy: 0.6420 - val_loss: 1.2827 - val_categorical_accuracy: 0.5635\n",
      "Epoch 11/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0194 - categorical_accuracy: 0.6530Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00011: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 289ms/step - loss: 1.0227 - categorical_accuracy: 0.6524 - val_loss: 1.3681 - val_categorical_accuracy: 0.5617\n",
      "Epoch 12/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9994 - categorical_accuracy: 0.6572Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00012: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 291ms/step - loss: 1.0018 - categorical_accuracy: 0.6579 - val_loss: 1.1839 - val_categorical_accuracy: 0.6174\n",
      "Epoch 13/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9859 - categorical_accuracy: 0.6553Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00013: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 0.9861 - categorical_accuracy: 0.6547 - val_loss: 1.3100 - val_categorical_accuracy: 0.5548\n",
      "Epoch 14/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0485 - categorical_accuracy: 0.6387Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00014: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 289ms/step - loss: 1.0459 - categorical_accuracy: 0.6398 - val_loss: 1.1593 - val_categorical_accuracy: 0.6296\n",
      "Epoch 15/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9700 - categorical_accuracy: 0.6699Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00015: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 291ms/step - loss: 0.9697 - categorical_accuracy: 0.6699 - val_loss: 1.3570 - val_categorical_accuracy: 0.5878\n",
      "Epoch 16/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0228 - categorical_accuracy: 0.6491Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00016: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 291ms/step - loss: 1.0253 - categorical_accuracy: 0.6492 - val_loss: 1.2772 - val_categorical_accuracy: 0.5791\n",
      "Epoch 17/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0166 - categorical_accuracy: 0.6367Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00017: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 292ms/step - loss: 1.0159 - categorical_accuracy: 0.6365 - val_loss: 1.1723 - val_categorical_accuracy: 0.6226\n",
      "Epoch 18/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0400 - categorical_accuracy: 0.6419Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00018: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 1.0400 - categorical_accuracy: 0.6417 - val_loss: 1.1789 - val_categorical_accuracy: 0.6243\n",
      "Epoch 19/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9530 - categorical_accuracy: 0.6618Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00019: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 292ms/step - loss: 0.9526 - categorical_accuracy: 0.6621 - val_loss: 1.0779 - val_categorical_accuracy: 0.6539\n",
      "Epoch 20/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0027 - categorical_accuracy: 0.6562Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00020: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 1.0032 - categorical_accuracy: 0.6566 - val_loss: 1.1590 - val_categorical_accuracy: 0.6191\n",
      "Epoch 21/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9558 - categorical_accuracy: 0.6683Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00021: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 0.9587 - categorical_accuracy: 0.6680 - val_loss: 1.1907 - val_categorical_accuracy: 0.5930\n",
      "Epoch 22/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9678 - categorical_accuracy: 0.6553Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00022: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 291ms/step - loss: 0.9692 - categorical_accuracy: 0.6540 - val_loss: 1.2791 - val_categorical_accuracy: 0.5252\n",
      "Epoch 23/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9525 - categorical_accuracy: 0.6637Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00023: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 0.9514 - categorical_accuracy: 0.6641 - val_loss: 1.2020 - val_categorical_accuracy: 0.6209\n",
      "Epoch 24/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9961 - categorical_accuracy: 0.6536Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00024: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 55s 287ms/step - loss: 0.9936 - categorical_accuracy: 0.6537 - val_loss: 1.1304 - val_categorical_accuracy: 0.6487\n",
      "Epoch 25/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9807 - categorical_accuracy: 0.6644Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00025: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 288ms/step - loss: 0.9791 - categorical_accuracy: 0.6647 - val_loss: 1.2112 - val_categorical_accuracy: 0.6122\n",
      "Epoch 26/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9423 - categorical_accuracy: 0.6735Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00026: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 55s 284ms/step - loss: 0.9425 - categorical_accuracy: 0.6731 - val_loss: 1.1610 - val_categorical_accuracy: 0.6313\n",
      "Epoch 27/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9560 - categorical_accuracy: 0.6647Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00027: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 290ms/step - loss: 0.9554 - categorical_accuracy: 0.6654 - val_loss: 1.3490 - val_categorical_accuracy: 0.4852\n",
      "Epoch 28/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 1.0282 - categorical_accuracy: 0.6455Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00028: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 55s 287ms/step - loss: 1.0289 - categorical_accuracy: 0.6446 - val_loss: 1.4671 - val_categorical_accuracy: 0.5965\n",
      "Epoch 29/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9714 - categorical_accuracy: 0.6631Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00029: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 56s 291ms/step - loss: 0.9738 - categorical_accuracy: 0.6621 - val_loss: 1.4464 - val_categorical_accuracy: 0.5530\n",
      "Epoch 30/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9475 - categorical_accuracy: 0.6624Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00030: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 55s 287ms/step - loss: 0.9464 - categorical_accuracy: 0.6631 - val_loss: 1.2020 - val_categorical_accuracy: 0.6087\n",
      "Epoch 31/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9403 - categorical_accuracy: 0.6732Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00031: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 61s 314ms/step - loss: 0.9410 - categorical_accuracy: 0.6735 - val_loss: 1.2783 - val_categorical_accuracy: 0.5948\n",
      "Epoch 32/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9770 - categorical_accuracy: 0.6582Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00032: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 302ms/step - loss: 0.9744 - categorical_accuracy: 0.6589 - val_loss: 1.2720 - val_categorical_accuracy: 0.6104\n",
      "Epoch 33/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9068 - categorical_accuracy: 0.6771Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00033: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 60s 310ms/step - loss: 0.9059 - categorical_accuracy: 0.6777 - val_loss: 1.1922 - val_categorical_accuracy: 0.6139\n",
      "Epoch 34/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9609 - categorical_accuracy: 0.6615Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00034: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 306ms/step - loss: 0.9599 - categorical_accuracy: 0.6621 - val_loss: 1.2051 - val_categorical_accuracy: 0.6296\n",
      "Epoch 35/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9742 - categorical_accuracy: 0.6631Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00035: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 307ms/step - loss: 0.9732 - categorical_accuracy: 0.6634 - val_loss: 1.1531 - val_categorical_accuracy: 0.6487\n",
      "Epoch 36/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9421 - categorical_accuracy: 0.6768Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00036: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 306ms/step - loss: 0.9407 - categorical_accuracy: 0.6770 - val_loss: 1.1238 - val_categorical_accuracy: 0.6522\n",
      "Epoch 37/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9160 - categorical_accuracy: 0.6849Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00037: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 308ms/step - loss: 0.9147 - categorical_accuracy: 0.6848 - val_loss: 1.2203 - val_categorical_accuracy: 0.6157\n",
      "Epoch 38/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8841 - categorical_accuracy: 0.6934Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00038: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 307ms/step - loss: 0.8819 - categorical_accuracy: 0.6936 - val_loss: 1.3521 - val_categorical_accuracy: 0.6035\n",
      "Epoch 39/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9350 - categorical_accuracy: 0.6745Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00039: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 60s 309ms/step - loss: 0.9328 - categorical_accuracy: 0.6754 - val_loss: 1.4906 - val_categorical_accuracy: 0.5635\n",
      "Epoch 40/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9459 - categorical_accuracy: 0.6719Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00040: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 307ms/step - loss: 0.9447 - categorical_accuracy: 0.6722 - val_loss: 1.2971 - val_categorical_accuracy: 0.5878\n",
      "Epoch 41/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9065 - categorical_accuracy: 0.6813Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00041: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 63s 327ms/step - loss: 0.9060 - categorical_accuracy: 0.6819 - val_loss: 1.2493 - val_categorical_accuracy: 0.6139\n",
      "Epoch 42/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9107 - categorical_accuracy: 0.6774Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00042: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 61s 318ms/step - loss: 0.9095 - categorical_accuracy: 0.6774 - val_loss: 1.3846 - val_categorical_accuracy: 0.5965\n",
      "Epoch 43/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9274 - categorical_accuracy: 0.6800Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00043: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 307ms/step - loss: 0.9280 - categorical_accuracy: 0.6793 - val_loss: 1.5204 - val_categorical_accuracy: 0.5635\n",
      "Epoch 44/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9505 - categorical_accuracy: 0.6647Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00044: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.9499 - categorical_accuracy: 0.6647 - val_loss: 1.2330 - val_categorical_accuracy: 0.5930\n",
      "Epoch 45/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9136 - categorical_accuracy: 0.6868Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00045: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 307ms/step - loss: 0.9123 - categorical_accuracy: 0.6877 - val_loss: 1.1939 - val_categorical_accuracy: 0.6400\n",
      "Epoch 46/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8953 - categorical_accuracy: 0.6833Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00046: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 302ms/step - loss: 0.8942 - categorical_accuracy: 0.6835 - val_loss: 1.1810 - val_categorical_accuracy: 0.6574\n",
      "Epoch 47/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9447 - categorical_accuracy: 0.6836Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00047: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 303ms/step - loss: 0.9434 - categorical_accuracy: 0.6832 - val_loss: 1.2350 - val_categorical_accuracy: 0.6400\n",
      "Epoch 48/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9274 - categorical_accuracy: 0.6748Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00048: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 302ms/step - loss: 0.9259 - categorical_accuracy: 0.6754 - val_loss: 1.3411 - val_categorical_accuracy: 0.5652\n",
      "Epoch 49/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9210 - categorical_accuracy: 0.6777Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00049: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 302ms/step - loss: 0.9200 - categorical_accuracy: 0.6783 - val_loss: 1.4454 - val_categorical_accuracy: 0.5930\n",
      "Epoch 50/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9113 - categorical_accuracy: 0.6914Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00050: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 302ms/step - loss: 0.9078 - categorical_accuracy: 0.6926 - val_loss: 1.1957 - val_categorical_accuracy: 0.6435\n",
      "Epoch 51/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8983 - categorical_accuracy: 0.6872Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00051: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 304ms/step - loss: 0.8990 - categorical_accuracy: 0.6877 - val_loss: 1.3728 - val_categorical_accuracy: 0.5826\n",
      "Epoch 52/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9044 - categorical_accuracy: 0.6940Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00052: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 303ms/step - loss: 0.9054 - categorical_accuracy: 0.6936 - val_loss: 1.4160 - val_categorical_accuracy: 0.6330\n",
      "Epoch 53/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9583 - categorical_accuracy: 0.6628Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00053: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.9560 - categorical_accuracy: 0.6631 - val_loss: 1.1758 - val_categorical_accuracy: 0.6417\n",
      "Epoch 54/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9152 - categorical_accuracy: 0.6813Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00054: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 304ms/step - loss: 0.9148 - categorical_accuracy: 0.6816 - val_loss: 1.4437 - val_categorical_accuracy: 0.5478\n",
      "Epoch 55/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9580 - categorical_accuracy: 0.6758Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00055: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 304ms/step - loss: 0.9578 - categorical_accuracy: 0.6757 - val_loss: 1.5190 - val_categorical_accuracy: 0.5896\n",
      "Epoch 56/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9622 - categorical_accuracy: 0.6755Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00056: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 304ms/step - loss: 0.9618 - categorical_accuracy: 0.6757 - val_loss: 1.2613 - val_categorical_accuracy: 0.5930\n",
      "Epoch 57/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9045 - categorical_accuracy: 0.6924Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00057: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 60s 308ms/step - loss: 0.9042 - categorical_accuracy: 0.6916 - val_loss: 1.2858 - val_categorical_accuracy: 0.6313\n",
      "Epoch 58/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9106 - categorical_accuracy: 0.6908Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00058: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 58s 303ms/step - loss: 0.9111 - categorical_accuracy: 0.6900 - val_loss: 1.4595 - val_categorical_accuracy: 0.5496\n",
      "Epoch 59/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9801 - categorical_accuracy: 0.6689Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00059: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 304ms/step - loss: 0.9840 - categorical_accuracy: 0.6683 - val_loss: 1.8525 - val_categorical_accuracy: 0.5148\n",
      "Epoch 60/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9093 - categorical_accuracy: 0.6921Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00060: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 306ms/step - loss: 0.9099 - categorical_accuracy: 0.6916 - val_loss: 1.2652 - val_categorical_accuracy: 0.6174\n",
      "Epoch 61/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8779 - categorical_accuracy: 0.6982Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00061: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.8766 - categorical_accuracy: 0.6987 - val_loss: 1.2391 - val_categorical_accuracy: 0.6174\n",
      "Epoch 62/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8794 - categorical_accuracy: 0.7064Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00062: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.8771 - categorical_accuracy: 0.7068 - val_loss: 1.2487 - val_categorical_accuracy: 0.6452\n",
      "Epoch 63/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8461 - categorical_accuracy: 0.7122Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00063: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 306ms/step - loss: 0.8452 - categorical_accuracy: 0.7123 - val_loss: 1.3438 - val_categorical_accuracy: 0.5722\n",
      "Epoch 64/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8947 - categorical_accuracy: 0.6969Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00064: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.8934 - categorical_accuracy: 0.6971 - val_loss: 1.3282 - val_categorical_accuracy: 0.6104\n",
      "Epoch 65/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.9309 - categorical_accuracy: 0.6771Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00065: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.9336 - categorical_accuracy: 0.6764 - val_loss: 1.2656 - val_categorical_accuracy: 0.5809\n",
      "Epoch 66/100\n",
      "192/192 [============================>.] - ETA: 0s - loss: 0.8532 - categorical_accuracy: 0.6963Found 575 images belonging to 10 classes.\n",
      "\n",
      "Epoch 00066: saving model to training_hist_new/cp.ckpt\n",
      "193/192 [==============================] - 59s 305ms/step - loss: 0.8521 - categorical_accuracy: 0.6971 - val_loss: 1.1805 - val_categorical_accuracy: 0.6557\n",
      "Epoch 67/100\n",
      "182/192 [===========================>..] - ETA: 3s - loss: 0.9350 - categorical_accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Keras_worker_ForkPoolWorker-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ac5eba1b3fa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0muse_multiprocessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 callbacks=[cp_callback])\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\", line 828, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"<ipython-input-15-662393544ba2>\", line 33, in generate_generator\n",
      "    X,y = gen.next()\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 116, in next\n",
      "    return self._get_batches_of_transformed_samples(index_array)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py\", line 238, in _get_batches_of_transformed_samples\n",
      "    x = self.image_data_generator.apply_transform(x, params)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
      "    order=self.interpolation_order)\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
      "    cval=cval) for x_channel in x]\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
      "    cval=cval) for x_channel in x]\n",
      "  File \"/home/charles/.local/lib/python3.6/site-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
      "    output, order, mode, cval, None, None)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_generator,\n",
    "                                steps_per_epoch=trainset_size/batch_size,\n",
    "                                epochs = epochs,\n",
    "                                validation_data = val_generator,\n",
    "                                validation_steps = valset_size/batch_size,\n",
    "                                use_multiprocessing = True,\n",
    "                                shuffle=True,\n",
    "                                callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
